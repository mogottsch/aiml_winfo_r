---
editor_options: 
  markdown: 
    wrap: 72
---

# Linear Regression

## Tidymodels framework

WHY TIDYMODELS? - R is open source with many contributors - Syntax of
different methods ("engines") varies a lot - Tidymodels provides an
unified syntax to access various engines (no need to know the specific
syntax of underlying engine) - Example: [Tidymodels
Example](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

![](Figures/tidymodels.png){width="500px"}

![](Figures/tidymodels_process.png){width="500px"}

-   rsample - Different types of re-samples
-   recipes - Transformations for model data pre-processing
-   parnip - A common interface for model creation
-   yardstick - Measure model performance

Resources: - Getting started with tidymodels:
<https://www.tidymodels.org/start> - Tidy Modeling with R:
<https://www.tmwr.org>

```{r, echo=FALSE}
set.seed(1234)
select <- dplyr::select
```

This lab will go over how to perform *linear regression*. This will
include [simple linear regression] and [multiple linear regression] in
addition to how you can apply transformations to the predictors. This
chapter will use [parsnip](https://www.tidymodels.org/start/models/) for
model fitting and [recipes and
workflows](https://www.tidymodels.org/start/recipes/) to perform the
transformations.

## Libraries

We load tidymodels and ISLR and MASS for data sets.

```{r, message=FALSE}
library(MASS) # For Boston data set
library(tidymodels)
library(ISLR)
library(GGally)
library(broom)
library(dotwhisker)
library(performance)
library(funModeling)
```

## Simple linear regression

![](Figures/Overview_supervised_learning.png){width="500px"}

![](Figures/Linear_regression.png){width="500px"}

$$ Sales=\beta_0 + \beta_1 \times TV + \epsilon, $$ where b0 and b1 are
unknown parameters that represent intercept and slope. (e is the error
term.)

The `Boston` data set contains various statistics for 506 neighborhoods
in Boston. We will build a simple linear regression model that related
the *median value of owner-occupied homes (`medv`)* as the response with
a variable indicating the *percentage of the population that belongs to
a lower status (`lstat`)* as the predictor.

```{block, type='infobox'}
The `Boston` data set is quite outdated and contains some really unfortunate variables.
```

Load data

```{r}
data(Boston)
head(Boston)
??Boston        # get help
```

Exploratory data analysis

```{r}
basic_eda <- function(data)
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

basic_eda(Boston)
```

Check the correlation

```{r}
Boston %>%
  select( medv, lstat ) %>%
  ggpairs()
```

We start by creating a *parsnip specification* for a linear regression
model (we train a model without pre-processing anything).

![](Figures/tidymodels_process.png){width="500px"}

```{r}
lm_spec <- linear_reg() %>%   # linear regression
  set_mode("regression") %>%  # regression (or classification)
  set_engine("lm")            # method of estimation, here, ordinary least squares

show_engines("linear_reg")
```

While it is *unnecessary to set the mode for a linear regression* since
it can only be regression, we continue to do it in these labs to be
explicit.

The specification doesn't perform any calculations by itself. It is
*just a specification* of what we want to do.

```{r}
lm_spec
```

Once we have the specification we can `fit` it by *supplying a formula
expression* and the data we want to fit the model on. The formula is
written on the form `y ~ x` where *`y` is the name of the response* and
*`x` is the name of the predictors*. The names used in the formula
should match the names of the variables in the data set passed to
`data`.

```{r}
lm_fit <- lm_spec %>%
  fit(medv ~ lstat, data = Boston)

lm_fit
```

The result of this fit is a *parsnip model object*. This object contains
the underlying fit as well as some parsnip-specific information.

The `lm` object has a nice *`summary()` method* that shows more
information about the fit, including parameter estimates and lack-of-fit
statistics.

```{r}
lm_fit %>% 
  pluck("fit") %>%
  summary()
```

We can use packages from the [broom](https://broom.tidymodels.org/)
package to extract key information out of the model objects in *tidy
formats.*

the `tidy()` function returns the parameter estimates of a `lm` object

```{r}
tidy(lm_fit)
```

and `glance()` can be used to *extract the model statistics*.

```{r}
glance(lm_fit)
```

We can also plot the estimate (using a dot-and-whisker plot).

```{r}
tidy(lm_fit) %>%
  dwplot()

tidy(lm_fit) %>% 
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))
```

Suppose that we like the model fit and we want to *generate
predictions*, we would typically use the `predict()` function.

```{r}
# Say we want to predict the "median value of owner-occupied homes" (medv) for first observation that has lstat = 4.98

# b0 = 34.55; b1 = -0.95

# medv = b0 + b1 * lstat

34.55 + (-0.95)*4.98
```

```{r, error=TRUE}
predict(lm_fit)
```

But this produces an error when used on a parsnip model object. This is
happening because we need to *explicitly supply the data set that the
predictions* should be performed on via the `new_data` argument

```{r}
predict(lm_fit, new_data = Boston)
```

We can also return other types of predicts by specifying the `type`
argument. Setting `type = "conf_int"` *return a 95% confidence
interval.*

```{r}
predict(lm_fit, new_data = Boston, type = "conf_int")
```

```{block, type='infobox'}
Not all engines can return all types of predictions.
```

If you want to evaluate the *performance of a model*, you might want to
*compare the observed value and the predicted value* for a data set.

```{r}
bind_cols(
  predict(lm_fit, new_data = Boston),
  Boston ) %>% 
  select(medv, .pred)
??predict
```

You can get the same results using the `augment()` function to save you
a little bit of typing.

```{r}
augment(lm_fit, new_data = Boston) %>% 
  select(medv, .pred)
```

Now we plot the predictions.

```{r}
plot_data
```
```{r}
plot_data <-
  bind_cols(
    predict(lm_fit, new_data = Boston),
    predict(lm_fit, new_data = Boston, type = "conf_int"),
    Boston ) %>% 
  select(medv, lstat, .pred, .pred_lower, .pred_upper)

plot_data %>%
  ggplot(aes(x = lstat)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "median value of owner-occupied homes",
       x = "percentage of the population that belongs to a lower status (`lstat`)")
```

## Multiple linear regression

The multiple linear regression model can be fit in much the same way as
the [simple linear regression] model. The only difference is how we
specify the predictors. We are using the same formula expression
`y ~ x`, but we can *specify multiple values* by separating them with
`+`s.

![](Figures/Linear_regression.png){width="500px"}

$$ Y=\beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{Radio} + \beta_3 \times \text{Newspaper} + \epsilon $$

```{r}
lm_fit2 <- lm_spec %>% 
  fit(medv ~ lstat + age, data = Boston)

lm_fit2
```

Everything else works the same. From extracting parameter estimates

```{r}
tidy(lm_fit2)

lm_fit2 %>% 
  pluck("fit") %>%
  summary()
```

to predicting new values

```{r}
predict(lm_fit2, new_data = Boston)
```

A shortcut when using formulas is to use the form `y ~ .` which means;
set `y` as the response and *set the remaining variables as predictors*.
This is very useful if you have a lot of variables and you don't want to
type them out.

```{r}
lm_fit3 <- lm_spec %>% 
  fit(medv ~ ., data = Boston)

lm_fit3
```

Now we plot the predictions.

```{r}
plot_data <-
  bind_cols(
    predict(lm_fit3, new_data = Boston),
    predict(lm_fit3, new_data = Boston, type = "conf_int"),
    Boston ) %>% 
  select(medv, lstat, .pred, .pred_lower, .pred_upper)

plot_data %>%
  ggplot(aes(x = lstat)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "median value of owner-occupied homes",
       x = "percentage of the population that belongs to a lower status (`lstat`)")
```

Let's *compare the prediction* from single and multiple linear
regression

```{r}
bind_cols(
  predict(lm_fit, new_data = Boston),
  predict(lm_fit2, new_data = Boston),
  predict(lm_fit3, new_data = Boston),
  Boston ) %>% 
  select(medv, .pred...1, .pred...2, .pred...3)
```

## Interaction terms

Adding interaction terms are quite easy to do using formula expressions.
However, the syntax used to describe them isn't accepted by all engines
so we will go over *how to include interaction terms using recipes* as
well.

There are two ways on including an interaction term; `x:y` and `x * y`

-   `x:y` will include the interaction between `x` and `y`,
-   `x * y` will include the interaction between `x` and `y`, `x`, and
    `y`, e.i. it is short for `x:y + x + y`.

with that out of the way let expand `lm_fit2` by adding an interaction
term

```{r}
lm_fit4 <- lm_spec %>%
  fit(medv ~ lstat * age, data = Boston)

lm_fit4
```

note that the interaction term is named `lstat:age`.

## Recipes

Sometimes we want to *perform transformations, and we want those
transformations to be applied, as part of the model fit as a
pre-processing step*. We will use the recipes package for this task.

We use the `step_interact()` to specify the interaction term. Next, we
create *a workflow object* to combine the linear regression model
specification `lm_spec` with the pre-processing specification
`rec_spec_interact` which can then be fitted much like a parsnip model
specification.

```{r}
rec_spec_interact <- recipe(medv ~ lstat + age + indus, data = Boston) %>%
  step_interact(~ lstat:age) %>%
  step_interact(~ lstat:indus)

lm_wf_interact <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_spec_interact)

lm_wf_interact %>% fit(Boston)
```

Notice that since we specified the variables in the recipe we don't need
to specify them when fitting the workflow object. Furthermore, take note
of the name of the interaction term. `step_interact()` tries to avoid
special characters in variables.

## Non-linear transformations of the predictors

Much like we could use recipes to create interaction terms between
values are we able to apply transformations to individual variables as
well. If you are familiar with the dplyr package then you know how to
`mutate()` which works in much the same way using `step_mutate()`.

You would want to keep as much of the pre-processing inside recipes such
that the transformation will be applied consistently to new data.

```{r}
rec_spec_pow2 <- recipe(medv ~ lstat, data = Boston) %>%
  step_mutate(lstat2 = lstat ^ 2)

lm_wf_pow2 <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_spec_pow2)

lm_wf_pow2 %>% fit(Boston)
```

You don't have to hand-craft every type of linear transformation since
recipes have a bunch created already
[here](https://recipes.tidymodels.org/reference/index.html#section-step-functions-individual-transformations)
such as `step_log()` to take logarithms of variables.

```{r}
rec_spec_log <- recipe(medv ~ lstat, data = Boston) %>%
  step_log(lstat)

lm_wf_log <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_spec_log)

lm_wf_log %>% fit(Boston)
```

## Qualitative predictors

We will now turn our attention to the `Carseats` data set. We will
attempt to predict `Sales` of child car seats in 400 locations based on
a number of predictors. One of these variables is `ShelveLoc` which is a
qualitative predictor that indicates the quality of the shelving
location. `ShelveLoc` takes on three possible values

-   Bad
-   Medium
-   Good

If you pass such a variable to `lm()` it will read it and generate dummy
variables automatically using the following convention.

Load data

```{r}
data(Carseats)
head(Carseats)
??Carseats
```

Exploratory data analysis

```{r}
basic_eda <- function(data)
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

basic_eda(Carseats)
```

Check the correlation

```{r}
Carseats %>%
  ggpairs()
```

So we have no problems including qualitative predictors when using `lm`
as the engine.

```{r}
lm_spec %>% 
  fit(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
```

However, as with so many things, *we can not always guarantee that the
underlying engine knows how to deal with qualitative variables*. Recipes
can be used to handle this as well. The `step_dummy()` will perform the
same transformation of turning 1 qualitative with `C` levels into `C-1`
indicator variables.

While this might seem unnecessary right now, some of the engines, later
on, do not handle qualitative variables and this step would be
necessary. We are also using the `all_nominal_predictors()` selector to
*select all character and factor predictor variables*. This allows us to
*select by type* rather than having to type out the names.

```{r}
rec_spec <- recipe(Sales ~ ., data = Carseats) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Income:Advertising + Price:Age)

lm_wf <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_spec)

results <- lm_wf %>% fit(Carseats)
results
```

Now we plot the predictions.

```{r}
plot_data <-
  bind_cols(
  predict(results, new_data = Carseats),
  predict(results, new_data = Carseats, type = "conf_int"),
  Carseats ) %>% 
  select(Sales, Price, .pred, .pred_lower, .pred_upper)

plot_data %>%
  ggplot(aes(x = Price)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "Sales",
       x = "Price")
```
